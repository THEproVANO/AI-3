# Лабораторная работа по курсу "Искусственный интеллект"
# Многослойный персептрон

| Студент | *ФИО* |
|------|------|
| Группа  | *№* |
| Оценка 1 (свой фреймворк) | *8* |
| Оценка 2 (PyTorch/Tensorflow) | *8* |
| Проверил | Сошников Д.В. |

> *Не очень правильно использовать ошибку MSE для задачи классификации. Не приводится код для сравнения значений гиперпараметров.*
### Задание

Решить задачу классификации для датасетов MNIST, FashionMNIST, CIFAR-10 с помощью 1, 2 и 3-слойного персептрона. Попробовать разные передаточные функции, провести сравнительную оценку решений. Решение сделать двумя способами:
* "С нуля", на основе базовых операций библиотеки numpy. Решение желательно реализовать в виде библиотеки, пригодной для решения более широкго круга задач.
* На основе одного из существующих нейросетевых фреймворков, в соответствии с вариантом задания:
   1. PyTorch
   1. Tensorflow/Keras

> *Номер варианта вычисляется по формуле 1 + (N-1) mod 2, где N - номер студента в списке.*

Решение оформить в файлах [Solution_MyFramework.ipynb](Solution_MyFramework.ipynb) и [Solution.ipynb](Solution.ipynb). 
Отчет по работе и сравнение методов пишется в этом файле после задания.
### Критерии оценки

Первая часть лабораторной работы:

| Сделано | Баллы |
|---------|-------|
| Реализован однослойный персептрон, классифицирующий датасет с точностью >85% | 1 |
| Реализован многослойный персептрон, классифицирующий датасет с точностью >85% | 2 |
| Реализация сделана как библиотека для обучения сетей различных конфигураций, в соответствии с примером | 1 |
| Улучшена архитектура библиотеки, отдельно вынесены алгоритмы обучения, функции потерь | 3 |
| Проведено сравнение различных гиперпараметров, таких, как передаточные функции, число нейронов в промежуточных слоях, функции потерь, с графиками обучения и матрицами неточности | 2 |
| Проведен анализ для датасета FashionMNIST | 1 |

Вторая часть лабораторной работы:

| Сделано | Баллы |
|---------|-------|
| Реализован однослойный персептрон, классифицирующий датасет с точностью >85% | 1 |
| Реализован многослойный персептрон, классифицирующий датасет с точностью >85% | 2 |
| Реализация использует возможности базового фреймворка, включая работу с данными | 3 |
| Проведено сравнение различных гиперпараметров, таких, как передаточные функции, число нейронов в промежуточных слоях, функции потерь, с графиками обучения и матрицами неточности | 2 |
| Проведен анализ для датасета FashionMNIST | 1 |
| Проведен анализ для другого датасета с цветными картинками (CIFAR-10) | 1 |

## Отчёт по работе

В данной лабораторной работе я создал свой неросетевой фреймворк, а также воспользовался уже готовым, называемым Tensorflow. 

Я впервые создал свою собственную неросеть. Ее построение оказалось несложным, а вот обучение совсем не простым. Но я разобрался и спустя пару часов запрограммировал свою нейросеть. Для проверки ее работоспособности я использовал датасет MNIST с рукописными цифрами от 0 до 9. Обработав немного этот датасет, я дал его своей нейросети для обучения. Спустя час обчения (так как моя нейросеть довольно медленная из-за асинхронной работы) я получил итоговую точность в 0.86 процентов (на тестовой выборке). График получившегося изменения лосс функции можно увидеть на картинке result.jpg в папке solution. Это лучший результат. Если брать более объемную нейросеть, то время обучения становится слишком долгим, а более меньшую, точность обучения страдает. Так для нейросети с одним глубинным слоем в 10 нейронов точность оказалась 0.678 процента, а с двумя глубинными слоями по 10 нейронов около 0.728.  

Также я создал нейросеть с помощью фреймоврка Tensorflow, использовав все тот же датасет MNIST. Эксплоатировать данный фреймворк мне понравилось больше. Вперую очередь, из-за наличия GPU обучения, с помощью которого нейросеть училась вместо часа в моей фреймворке всего 10 секунд, с получившейся точностью около 0.95 процента. Также хочу заметить, что установка GPU обеспечения заняла у меня в несколько раз больше времени, нежели написание готовей рабочей нейросети на данном фреймворке. Также при работе с нейросетью, я использовал различные гиперпараметры и смотерел, как обучиться нейросеть в том или ином случае. Это исследование подробно распиано в JupterNotebook. Результатом я оказался доволен. 

В данной лабораторной работе я познакомился с глубоким устройством обычного персептрона, запрограмировал его простой случай и воспользовался уже готовыми сильными его реализациями. Мне понравилась эта лабораторная работа, так как я считаю, что Исскуственный интелекет и нейросети это будущее человечества и поэтому я хочу знать, как работают различные нейросети, и также уметь ими воспользоваться в любой момент. 

## Материалы для изучения

 * [Реализация своего нейросетевого фреймворка](https://github.com/shwars/NeuroWorkshop/blob/master/Notebooks/IntroMyFw.ipynb)
 * [Введение в PyTorch](https://github.com/shwars/NeuroWorkshop/blob/master/Notebooks/IntroPyTorch.ipynb)
 * [Введение в Tensorflow/Keras](https://github.com/shwars/NeuroWorkshop/blob/master/Notebooks/IntroKerasTF.ipynb)
