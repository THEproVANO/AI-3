{"cells":[{"source":["## Лабораторная работа по курсу Искусственный интеллект\n","## Многослойный персептрон\n","\n","| Студент | Купцов |\n","|---------|--------|\n","| Группа  | 7      |"],"cell_type":"markdown","metadata":{}},{"metadata":{"trusted":true},"cell_type":"code","source":["import numpy as np\n","from numpy import linalg as LA\n","import statistics as st"],"execution_count":1,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["Добавляем необходимые библиотеки"]},{"metadata":{"trusted":true},"cell_type":"code","source":["class Layer(object):\n","\tdef linear(self, x):\n","\t\treturn x\n","\tdef linear_d(self, x):\n","\t\treturn np.ones(x.shape)\n","\tdef relu(self, x):\n","\t\treturn np.where(x < 0, 0, x)\n","\tdef relu_d(self, x):\n","\t\treturn np.where(x < 0, 0, 1)\n","\tdef sigm(self, x):\n","\t\treturn 1.0/(1.0 + np.exp(-x))\n","\tdef sigm_d(self,x):\n","\t\treturn self.sigm(x)*(1 - self.sigm(x))\n","\tdef th(self, x):\n","\t\treturn np.tanh(x)\n","\tdef th_d(self, x):\n","\t\treturn 1.0 - self.th(x)**2\n","\tdef softmax(self, x):\n","\t\tu = np.exp(x - np.max(x))\n","\t\treturn u/np.sum(u)\n","\tdef softmax_d(self, x):\n","\t\treturn x*(1 - x)"],"execution_count":1,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["Создаем класс одного слоя. Объявляем функции активации и их производные."]},{"metadata":{"trusted":true},"cell_type":"code","source":["    def __init__(self, dim_in, activation='linear'):\n","\t\tif activation == 'sigm':\n","\t\t\tself.f = self.sigm\n","\t\t\tself.f_d = self.sigm_d\n","\t\telif activation == 'th':\n","\t\t\tself.f = self.th\n","\t\t\tself.f_d = self.th_d\n","\t\telif activation == 'relu':\n","\t\t\tself.f = self.relu\n","\t\t\tself.f_d = self.relu_d\n","\t\telif activation == 'softmax':\n","\t\t\tself.f = self.softmax\n","\t\t\tself.f_d = self.softmax_d\n","\t\telse:\n","\t\t\tself.f = self.linear\n","\t\t\tself.f_d = self.linear_d\n","\t\tself.dim = dim_in"],"execution_count":3,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["Добавляем конструктор, требующий размер и функцию активации для слоя"]},{"metadata":{"trusted":true},"cell_type":"code","source":["    def add(self, lr):\n","        self.weights = np.random.randn(lr.dim, self.dim)\n","        self.bias = np.random.randn(lr.dim, 1)"],"execution_count":4,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["Объявляем метод добавления слоя к другому слою, парралельно создавая случайные веса между ними"]},{"metadata":{"trusted":true},"cell_type":"code","source":["\tdef calculate(self, input):\n","\t\treturn np.dot(self.weights, input) + self.bias"],"execution_count":6,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["И наконец, завершаем объявление класса, добавляя функцию вычисления значений следующих нейронов слоя"]},{"metadata":{"trusted":true},"cell_type":"code","source":["class NN(object):\n","\tdef square(self, y_t, y_p):\n","\t\treturn 0.5*(y_t - y_p)**2\n","\tdef square_d(self, y_t, y_p):\n","\t\t\treturn (y_p - y_t)\n","\tdef cross_entropy(self, y_t, y_p):\n","\t\treturn -y_t*np.log(y_p)\n","\tdef cross_entropy_d(self, y_t, y_p):\n","\t\treturn -y_t/y_p"],"execution_count":7,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["Теперь создаем класс самой нейросети, сразу добавив функции потерь и их производные"]},{"metadata":{"trusted":true},"cell_type":"code","source":["    def __init__(self):\n","        self.layers = []\n","    def add_layer(self, lr):\n","        self.layers.append(lr)\n","        if len(self.layers) != 1:\n","            self.layers[-2].add(lr)"],"execution_count":8,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["Объявляем простой конструктор и метод для накопления нейросетью слоев"]},{"metadata":{"trusted":true},"cell_type":"code","source":["    def answer(self, input):\n","\t\tif len(self.layers) == 0 or len(self.layers) == 1:\n","\t\t\traise ValueError(\"bad NN\")\n","\t\tif len(input) != self.layers[0].dim:\n","\t\t\traise ValueError(\"size input error\")\n","\t\ty = np.array(input)\n","\t\tself.answers = [0]*(len(self.layers))\n","\t\tself.answers[0] = np.array(y)\n","\t\ty.shape = (len(y), 1)\n","\t\tfor i in range(len(self.layers) - 1):\n","\t\t\ty = self.layers[i].calculate(y)\n","\t\t\ty = self.layers[i + 1].f(y)\n","\t\t\tself.answers[i + 1] = y.T[0]\n","\t\treturn y.T[0]"],"execution_count":10,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["Функция высчитывающая все значения слоев нейросети"]},{"metadata":{"trusted":true},"cell_type":"code","source":["    def fit(self, x, y, loss_in):\n","\t\tif len(x) != len(y):\n","\t\t\traise ValueError(\"bad data\")\n","\t\tif loss_in == 'sqr':\n","\t\t\tself.loss = self.square\n","\t\t\tself.loss_d = self.square_d\n","\t\telif loss_in == 'cross_entropy':\n","\t\t\tself.loss = self.cross_entropy\n","\t\t\tself.loss_d = self.cross_entropy_d\n","\t\telse:\n","\t\t\traise SyntaxError(\"no function\")\n","\t\tn = 0.001\n","\t\tlosses = [0]*len(x)\n","\t\tfor data in range(len(x)):\n","\t\t\tself.answer(x[data])\n","\t\t\tif len(y[data]) != self.layers[-1].dim:\n","\t\t\t\traise ValueError(\"bad data\")\n","\t\t\tlosses[data] = LA.norm(self.loss(y[data], self.answers[-1]), 2)\n","\t\t\tsigr = self.loss_d(y[data], self.answers[-1])*self.layers[-1].f_d(self.answers[-1])\n","\t\t\tfor j in range(len(self.layers) - 2, -1, -1):\n","\t\t\t\tlay_sig = np.zeros((self.layers[j].dim))\n","\t\t\t\tfor l in range(self.layers[j].dim):\n","\t\t\t\t\tu = 0\n","\t\t\t\t\tfor r in range(self.layers[j + 1].dim):\n","\t\t\t\t\t\tu += self.layers[j].weights[r][l]*sigr[r]\n","\t\t\t\t\tlay_sig[l] = u*self.layers[j + 1].f_d(self.answers[j][l])\n","\t\t\t\tfor l in range(self.layers[j].dim):\n","\t\t\t\t\tfor r in range(self.layers[j + 1].dim):\n","\t\t\t\t\t\tself.layers[j].weights[r][l] -= n*sigr[r]*self.answers[j][l]\n","\t\t\t\tfor r in range(self.layers[j + 1].dim):\n","\t\t\t\t\tself.layers[j].bias[r] -= n*sigr[r]\n","\t\t\t\tsigr = lay_sig\n","\t\treturn st.median(losses)"],"execution_count":11,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["И наконец, главный метод обучения нейросети.\n","\n","Получая на вход данные, нейросеть учится, посредством обратного распротранения ошибки."]},{"metadata":{"trusted":true},"cell_type":"code","source":["    def test(self, x, y):\n","\t\tif len(x) != len(y):\n","\t\t\traise ValueError(\"bad data\")\n","\t\tacc = 0\n","\t\tfor data in range(len(x)):\n","\t\t\tself.answer(x[data])\n","\t\t\tif len(y[data]) != self.layers[-1].dim:\n","\t\t\t\traise ValueError(\"bad data\")\n","\t\t\tfor i in range(len(y[data])):\n","\t\t\t\tif y[data][i] == 1:\n","\t\t\t\t\tmax = float(\"-inf\")\n","\t\t\t\t\tfor j in range(len(y[data])):\n","\t\t\t\t\t\tif self.answers[-1][j] > max:\n","\t\t\t\t\t\t\tmax = self.answers[-1][j]\n","\t\t\t\t\t\t\tindex = j\n","\t\t\t\t\tif index == i:\n","\t\t\t\t\t\tacc += 1\n","\t\t\t\t\tbreak\n","\t\tprint(\"accuracy: \", float(acc)/len(x))"],"execution_count":12,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["Последний метод, тестирующий получившуюся нейронную сеть на тестовой выборке.\n","\n","Так выглядит моя библиотека для работы с нейросетями.\n","\n","Далее, мы будем ее использовать\n"]},{"metadata":{"trusted":true},"cell_type":"code","source":["!conda install keras\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from keras.datasets import mnist"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["Теперь протестируем нашу нейросеть. Используем базу данных mnist"]},{"metadata":{"trusted":true},"cell_type":"code","source":["(x_train, y_train), (x_test, y_test) = mnist.load_data()"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["Закачиваем данные"]},{"metadata":{"trusted":true},"cell_type":"code","source":["Y = [0]*len(y_train)\n","for i in range(len(y_train)):\n","\tans = y_train[i]\n","\tY[i] = np.zeros((10,))\n","\tY[i][ans] = 1\n","\n","for i in range(len(x_train)):\n","\tfor j in range(28*28):\n","\t\tx_train[i][j] /= 255.0\n","X = x_train"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["Подгоняем эти данные"]},{"metadata":{"trusted":true},"cell_type":"code","source":["model = NN()\n","model.add_layer(Layer(28*28))\n","model.add_layer(Layer(20, 'sigm'))\n","model.add_layer(Layer(10, 'softmax'))"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["Создаем нейросеть с входным слоем размера 784 нейрона, одного внутреннего слоя с 20 нейронами и с функцией активации сигмоида, а также выходной слой размером 10 нейронов и с функцией активации softmax."]},{"metadata":{"trusted":true},"cell_type":"code","source":["start = 0\n","step = 1000\n","end = step\n","points = []\n","for i in range(len(X)//step):\n","\tpoints.append(model.fit(X[start:end], Y[start:end], 'sqr'))\n","\tstart += step\n","\tend += step"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["Обучаем нейросеть одной эпохой в несколько батчей размером с 1000 данных"]},{"metadata":{"trusted":true},"cell_type":"code","source":["model.test(X[0:1000], Y[0:1000])\n","\n","plt.plot(points)\n","plt.title('Loss')\n","plt.show()"],"execution_count":null,"outputs":[]},{"source":["Тестируем точность и рисуем графики"],"cell_type":"code","metadata":{},"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["![](solution/result.jpg)"]},{"metadata":{},"cell_type":"markdown","source":["В итоге, получили неплохой результат"]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}